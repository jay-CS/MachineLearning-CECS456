{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression testing...\n",
      "max iteration testcase0: Train accuracy: 0.834721, Test accuracy: 0.827830\n",
      "max iteration testcase1: Train accuracy: 0.924407, Test accuracy: 0.900943\n",
      "max iteration testcase2: Train accuracy: 0.966047, Test accuracy: 0.941038\n",
      "max iteration testcase3: Train accuracy: 0.973735, Test accuracy: 0.950472\n",
      "learning rate testcase0: Train accuracy: 0.966047, Test accuracy: 0.941038\n",
      "learning rate testcase1: Train accuracy: 0.973735, Test accuracy: 0.950472\n",
      "learning rate testcase2: Train accuracy: 0.978860, Test accuracy: 0.962264\n",
      "logistic regression test done.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\" \n",
    "In the logistic regression function we are passing in a data set numpy array of shape(n,3) along with the labels for \n",
    "each entry in the dataset. With this in mind, the function is initializing the weight vector and for each iteration \n",
    "specified in the ‘max_iteration’ parameter, a gradient descent vector will be initialized that will traverse the \n",
    "entire data set for each iteration in max_iteration. In total O(max_iter*len(dataset)). After traversing the list and \n",
    "obtaining the gradient descent vector, it will be multiplied by theta (learning_rate) and subtracted from the weight \n",
    "vector to optimize it. As a result, this procedure of iterating through the entire data for a total of N times, \n",
    "produces a weight that when multiplied by a sample in X dataset (dot product) and plugged in the sigmoid function, \n",
    "will likely produce a probability towards correctly identifying the sample. Downside to this function is that it \n",
    "takes extremely long to execute as a batch of 1561 samples will be computed anywhere from 156,100 to 1,561,000 times. \n",
    "It takes minutes to run, but ultimately converges towards 98% accuracy in the end.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_logistic_regression()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression SGD testing...\n",
      "max iteration testcase0: Train accuracy: 0.905189, Test accuracy: 0.879717\n",
      "max iteration testcase1: Train accuracy: 0.918642, Test accuracy: 0.889151\n",
      "max iteration testcase2: Train accuracy: 0.921845, Test accuracy: 0.893868\n",
      "max iteration testcase3: Train accuracy: 0.977578, Test accuracy: 0.966981\n",
      "learning rate testcase0: Train accuracy: 0.956438, Test accuracy: 0.941038\n",
      "learning rate testcase1: Train accuracy: 0.970532, Test accuracy: 0.948113\n",
      "learning rate testcase2: Train accuracy: 0.978219, Test accuracy: 0.959906\n",
      "logistic regression SGD test done.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This function is generally the same as the regular Gradient Descent algorithm, instead of traversing the entire \n",
    "dataset for N times. The function will iterate through N random points, and for each point N in the dataset X, it \n",
    "will calculate the gradient at that point, and multiply it by some , and subtract this product from the weight \n",
    "vector to optimize it. Overall this algorithm is extremely efficient in terms of O(N random points), but may suffer \n",
    "extremely in the initial test cases as seen by the first test case with 64% accuracy, but does converge towards 98%\n",
    "accuracy.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_logistic_regression()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3rd order logistic regression testing...\n",
      "max iteration testcase0: Train accuracy: 0.924407, Test accuracy: 0.898585\n",
      "max iteration testcase1: Train accuracy: 0.958360, Test accuracy: 0.941038\n",
      "max iteration testcase2: Train accuracy: 0.970532, Test accuracy: 0.948113\n",
      "max iteration testcase3: Train accuracy: 0.975016, Test accuracy: 0.955189\n",
      "learning rate testcase0: Train accuracy: 0.970532, Test accuracy: 0.948113\n",
      "learning rate testcase1: Train accuracy: 0.975016, Test accuracy: 0.955189\n",
      "learning rate testcase2: Train accuracy: 0.978219, Test accuracy: 0.964623\n",
      "3rd order logistic regression test done.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "In the third order function  we are taking in the data samples of shape (n,3), and transforming it into a Z space of \n",
    "(n,10) dimension. In order to peform this a z_x numpy array will be initialized and each entry in the z_x will be a \n",
    "list of 10 real number produced by the following equation \n",
    "[1,x1,x2] = [1, x1, x2, (x1)^2, (x1)(x2),(x2)^2, (x1)^3, (x1)^2 (x2), (x1) (x2)^2, (x2)^3 ].\n",
    "Why? Because by transforming this into a polynomial of degree 3 we are hoping to get a  feature space that is \n",
    "linearly separable or close to it. However, we see that using a third degree polynomial transform is only slightly \n",
    "better after the first two test cases when training and testing our model.\n",
    "\n",
    "\"\"\"\n",
    "if __name__ == '__main__':\n",
    "    test_thirdorder_logistic_regression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3rd order logistic regression SGD testing...\n",
      "max iteration testcase0: Train accuracy: 0.782832, Test accuracy: 0.794811\n",
      "max iteration testcase1: Train accuracy: 0.930814, Test accuracy: 0.900943\n",
      "max iteration testcase2: Train accuracy: 0.949391, Test accuracy: 0.919811\n",
      "max iteration testcase3: Train accuracy: 0.973735, Test accuracy: 0.950472\n",
      "learning rate testcase0: Train accuracy: 0.970532, Test accuracy: 0.950472\n",
      "learning rate testcase1: Train accuracy: 0.975657, Test accuracy: 0.952830\n",
      "learning rate testcase2: Train accuracy: 0.977578, Test accuracy: 0.952830\n",
      "3rd order logistic regression SGD test done.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "3rd Order with SGD: This function performs the exact same computation as the regular third order function, the only difference is that \n",
    "when the x feature space is transformed into the z feature space, the z feature space will find an optimal using the \n",
    "SGD algorithm rather than the regular Gradient Descent.\n",
    "\n",
    "\"\"\"\n",
    "if __name__ == '__main__':\n",
    "\ttest_thirdorder_logistic_regression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
